{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48ea98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "import json\n",
    "from pyserini.index import IndexReader\n",
    "from pyserini.search import SimpleSearcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60992ac",
   "metadata": {},
   "source": [
    "**Note**: The query id in PLN is implemented in a slightly redundant way, but the code for task 3 and task 4 is very similar and improved upon. For PLN, I did not consider the document title only the document description. For other tasks I merged the the text in Title and Description. I have used a separte python file to convert csv to json and I merge the csv columns : Title and Description(for documents) in there only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb4326c",
   "metadata": {},
   "source": [
    "# Task 2: Implement Pivoted Length Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895ce4d3",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35290d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59887 entries, 0 to 59886\n",
      "Data columns (total 4 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   DocumentId            59887 non-null  int64 \n",
      " 1   Title                 59724 non-null  object\n",
      " 2   Date                  59879 non-null  object\n",
      " 3   Document Description  48757 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "documents_df = pd.read_csv('hw2/documents.csv')\n",
    "documents_df = documents_df[documents_df['Document Description'].notnull()]\n",
    "queries = pd.read_csv('hw2/query.csv')\n",
    "queries.head(), documents_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb809431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the doc and query ids\n",
    "document_id = documents_df.DocumentId.astype(str).values.tolist()\n",
    "query_id = queries.QueryId.astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedbba7f",
   "metadata": {},
   "source": [
    "#### This section precalculates the length of the documents and stores it in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cc0236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the documents and get their length\n",
    "# doc_length = {}\n",
    "# for docid in document_id[:10]:\n",
    "#     doc_vector = index_reader.get_document_vector(docid)\n",
    "#     document_length[docid] = sum(doc_vector.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71577c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the doc lengths of each doc in a json file\n",
    "filename= 'hw2/document_length'\n",
    "# with open(filename, 'w') as outfile:\n",
    "#     json.dump(document_length, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57cd2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the \n",
    "with open(filename) as infile:\n",
    "    document_length = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de039eec",
   "metadata": {},
   "source": [
    "#### Get Index reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "368f2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the index files for the documents and the querires\n",
    "index_reader = IndexReader('hw2/documents')\n",
    "query_index_reader = IndexReader('hw2/query')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbce6dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145.28287220296573, 48757)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = index_reader.stats().get('non_empty_documents')\n",
    "avg_dl = index_reader.stats().get('total_terms')/ N\n",
    "avg_dl,N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5765d568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PivotedLengthNormalizationRanker(query_id, b=0.3):\n",
    "    ''' Implements the PLN formula to score relevance\n",
    "    Input: \n",
    "        QueryId, b(hyperparameter) - default value =0.3\n",
    "    \n",
    "    Output: \n",
    "        common_term: A nested dict with doc_id as key and common terms \n",
    "            between query and repective score as another dict\n",
    "            Sample: {'doc_id1':{'how': 2.3, 'gain': 3},'doc_id2':{'tell': 2.3, 'level': 3} }\n",
    "    '''\n",
    "    \n",
    "    #get the query contents\n",
    "    query = query_index_reader.doc(query_id).raw()\n",
    "    query= json.loads(query)['contents']\n",
    "    query_vector = query_index_reader.get_document_vector(query_id)\n",
    "    \n",
    "    # get analyzed terms\n",
    "    analyzed_query = index_reader.analyze(query)\n",
    "    \n",
    "    #loop over the analyzed terms\n",
    "    for terms in analyzed_query:\n",
    "        common_term = {}\n",
    "        #get the documents which contain the query term\n",
    "        postings_list = index_reader.get_postings_list(terms, analyzer=None)\n",
    "        # find the tf of this term in query_vector\n",
    "        tf_query = query_vector.get(terms) \n",
    "        \n",
    "        # check the document contains the word and the term is not an empty string\n",
    "        if postings_list is not None and tf_query is not None:\n",
    "            \n",
    "            df_word = len(postings_list)  # the no.of documents that contain the word\n",
    "        \n",
    "            # calculate the score for each document and word\n",
    "            for posting in postings_list:\n",
    "                tf_doc= posting.tf # c(w,d)\n",
    "                #get doc id for the index reader\n",
    "                docsid = posting.docid\n",
    "                # get the doc_id corresponding to lucene index\n",
    "                doc_id = index_reader.convert_internal_docid_to_collection_docid(docsid)\n",
    "                doc_length= document_length.get(doc_id) #get doc length       \n",
    "                \n",
    "                # calculate the PLN score for one word in query\n",
    "                idf = np.log((N+1)/df_word)\n",
    "                normalized_tf = (1 + np.log(1 + np.log(tf_doc)))/(1- b + b * doc_length/avg_dl)\n",
    "                score = tf_query * normalized_tf * idf\n",
    "\n",
    "                #check if doc_id is already in the dict\n",
    "                if doc_id in common_term.keys():\n",
    "                    # summation of each word in query for a doc_id\n",
    "                    common_term[doc_id] = common_term[doc_id] + score  \n",
    "                else:\n",
    "                    common_term[doc_id] = score\n",
    "    \n",
    "    return common_term\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c68c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rank = []\n",
    "# loop through the query ids\n",
    "for q_id in query_id:\n",
    "    #call the PLN ranker\n",
    "    result = PivotedLengthNormalizationRanker(q_id)\n",
    "    \n",
    "    #sort the result as per score and take top 10 documents\n",
    "    documents = dict(sorted(result.items(), key=lambda item: item[1], reverse=True)[:10]).keys()\n",
    "    \n",
    "    # pair the document id with the query id\n",
    "    for doc_id in documents:\n",
    "        final_rank.append([q_id, doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f0c7d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists to df and save as csv\n",
    "df =pd.DataFrame(final_rank, columns= ['QueryId', 'DocumentId'])\n",
    "df.to_csv('hw2/seffendi_pln.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "ffd3eace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350 entries, 0 to 349\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   QueryId     350 non-null    int64\n",
      " 1   DocumentId  350 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 5.6 KB\n"
     ]
    }
   ],
   "source": [
    "sample =pd.read_csv('hw2/sample_submission.csv')\n",
    "sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f249ac0",
   "metadata": {},
   "source": [
    "# Task 3: Implement and Tune BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed650081",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b62919f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1595 entries, 0 to 1594\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   QueryId            1595 non-null   int64 \n",
      " 1   Query Description  1595 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 25.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 45300 entries, 0 to 45300\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   DocumentId            45300 non-null  int64 \n",
      " 1   Document Title        45300 non-null  object\n",
      " 2   Document Description  45300 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_gaming = pd.read_csv('hw2/documents_gaming.csv')\n",
    "documents_gaming = documents_gaming[documents_gaming['Document Description'].notnull()]\n",
    "queries_gaming = pd.read_csv('hw2/query_gaming.csv')\n",
    "queries_gaming.info(), documents_gaming.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "47f1ed37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the doc and query ids as lists\n",
    "document_id_gaming = documents_gaming.DocumentId.astype(str).values.tolist()\n",
    "query_id_gaming = queries_gaming.QueryId.astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2aa4ab4",
   "metadata": {},
   "source": [
    "#### Get Index reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b6c0dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the index files for the documents and the querires\n",
    "index_reader_gaming = IndexReader('hw2/gaming_documents2')\n",
    "query_index_reader_gaming = IndexReader('hw2/gaming_query')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c3b083",
   "metadata": {},
   "source": [
    "#### This section gets the sample query ids and then saves the words in each query and its tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f6596380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_gaming_sample = pd.read_csv('hw2/gaming_query_sample_submission.csv')\n",
    "# queries_gaming_sample.info()#, queries_gaming.info()\n",
    "queries_from_sample = list(set(queries_gaming_sample.QueryId.tolist()))\n",
    "len(queries_from_sample)\n",
    "# queries_from_sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "db2e7285",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over the queries and get their words and their tf\n",
    "all_queries={}\n",
    "for queryid in queries_from_sample:\n",
    "    query_vector = query_index_reader_gaming.get_document_vector(str(queryid))\n",
    "    all_queries[str(queryid)] = query_vector\n",
    "    \n",
    "len(all_queries.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551df52",
   "metadata": {},
   "source": [
    "#### This section precalculates the length of the documents and stores it in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "8547069b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the documents and get their length\n",
    "# document_length_gaming = {}\n",
    "# for docid in document_id_gaming:\n",
    "#     doc_vector = index_reader_gaming.get_document_vector(docid)\n",
    "#     document_length_gaming[docid] = sum(doc_vector.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b449aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the doc lengths of each doc in a json file\n",
    "filename= 'hw2/document_length_gaming'\n",
    "# with open(filename, 'w') as outfile:\n",
    "#     json.dump(document_length_gaming, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "41fd73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file\n",
    "with open(filename) as infile:\n",
    "    document_length_gaming = json.load(infile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36137830",
   "metadata": {},
   "source": [
    "#### Calculate no.of documents and the average length of documents in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "966753ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87.8650360919185, 45301)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_gaming = index_reader_gaming.stats().get('non_empty_documents')\n",
    "avg_dl_gaming = index_reader_gaming.stats().get('total_terms')/ N_gaming\n",
    "avg_dl_gaming,N_gaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "783c55b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25Ranker(words_dict, k1=1.2, b=0.75, k3=1.2):\n",
    "    ''' Implements the bm25 formula to score relevance\n",
    "    Input: \n",
    "        words_dict: It is a dict containing analyzed terms of a query as keys \n",
    "                    and its term freq in the query as value\n",
    "        hyperparameters: default values given\n",
    "                k1 : 1.2\n",
    "                b  : 0.75\n",
    "                k3 : 1.2\n",
    "    Output: \n",
    "        commom_term_bm25: A nested dict with doc_id as key and common terms \n",
    "                        between query and repective score as another dict\n",
    "                        Sample: {'doc_id1':{'how': 2.3, 'gain': 3},'doc_id2':{'tell': 2.3, 'level': 3} }\n",
    "    '''\n",
    "    common_term_bm25 = {}\n",
    "    #loop over the words in the words_dict \n",
    "    for word,qtf in words_dict.items():\n",
    "\n",
    "        postings_list = index_reader_gaming.get_postings_list(word, analyzer=None)\n",
    "        \n",
    "        if postings_list is not None :\n",
    "            word_df = len(postings_list)\n",
    "\n",
    "            for posting in postings_list:\n",
    "                doc_tf= posting.tf # c(w,d)\n",
    "                docsid = posting.docid\n",
    "                # get the doc_id corresponding to lucene index\n",
    "                doc_id = index_reader_gaming.convert_internal_docid_to_collection_docid(docsid)\n",
    "                doc_len_gaming= document_length_gaming.get(doc_id) #get doc length \n",
    "                \n",
    "                #check the length is not none\n",
    "                if doc_len_gaming:\n",
    "                #calculate the BM25 score for one word in query\n",
    "                    normalized_qtf = ((k3+1)* qtf)/(k3 + qtf)\n",
    "                    normalized_tf = ((k1 +1)*doc_tf)/((k1*(1-b+ b*(doc_len_gaming/avg_dl_gaming)))+doc_tf)\n",
    "                    variant_idf = np.log((N_gaming - word_df+0.5)/(word_df+0.5))\n",
    "                    score = variant_idf * normalized_tf *normalized_qtf\n",
    "\n",
    "                    #check if doc_id is already in the dict\n",
    "                    if doc_id in common_term_bm25.keys():\n",
    "                        # summation of each word in query for a doc_id\n",
    "                        common_term_bm25[doc_id] = common_term_bm25[doc_id] + score \n",
    "                    else:\n",
    "                        common_term_bm25[doc_id] = score\n",
    "    return common_term_bm25\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4fb815c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rank_gaming = []\n",
    "# loop through the query ids\n",
    "# pdb.set_trace()\n",
    "for qid,words in all_queries.items():\n",
    "    #call the BM25 ranker\n",
    "    result = BM25Ranker(words, k1=1.2, b=0.76, k3=1.25)\n",
    "    \n",
    "    #sort the result as per score and take top 5 documents\n",
    "    documents_gaming_rank = dict(sorted(result.items(), key=lambda item: item[1], reverse=True)[:5]).keys()\n",
    "    # pair the document id with the query id\n",
    "    for doc_id in documents_gaming_rank:\n",
    "        final_rank_gaming.append([qid, doc_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c2ceb244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert list of lists to df and save as csv\n",
    "df_bm25 =pd.DataFrame(final_rank_gaming, columns= ['QueryId', 'DocumentId'])\n",
    "df_bm25.to_csv('hw2/seffendi_bm25_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "308a63af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 165 entries, 0 to 164\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   QueryId     165 non-null    object\n",
      " 1   DocumentId  165 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 2.7+ KB\n"
     ]
    }
   ],
   "source": [
    "df_bm25.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f003ddb",
   "metadata": {},
   "source": [
    "# Task4: Design Your Own Scoring Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a2bdcd",
   "metadata": {},
   "source": [
    "### Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb7578ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 699 entries, 0 to 698\n",
      "Data columns (total 2 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   QueryId            699 non-null    int64 \n",
      " 1   Query Description  699 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 22998 entries, 0 to 22997\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   DocumentId            22998 non-null  int64 \n",
      " 1   Document Title        22998 non-null  object\n",
      " 2   Document Description  22998 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 539.1+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 255 entries, 0 to 254\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype\n",
      "---  ------      --------------  -----\n",
      " 0   QueryId     255 non-null    int64\n",
      " 1   DocumentId  255 non-null    int64\n",
      "dtypes: int64(2)\n",
      "memory usage: 4.1 KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None, None)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_android = pd.read_csv('hw2/ranker/documents_android.csv')#, index_col= 'DocumentId'\n",
    "# documents_android = documents_gaming[documents_gaming['Document Description'].notnull()]\n",
    "queries_android = pd.read_csv('hw2/ranker/query_android.csv')\n",
    "queries_android_sample = pd.read_csv('hw2/ranker/android_query_sample_submission.csv')\n",
    "queries_android.info(), documents_android.info(), queries_android_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc56f7e",
   "metadata": {},
   "source": [
    "### Get the Index reader for documents and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88170bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the index files for the documents and the querires\n",
    "index_reader_android = IndexReader('hw2/ranker/android_documents')\n",
    "query_index_reader_android = IndexReader('hw2/ranker/android_query')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496df365",
   "metadata": {},
   "source": [
    "### Calculations to make processing faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2bd2d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the list of doc and query ids\n",
    "document_id_android = documents_android.DocumentId.astype(str).values.tolist()\n",
    "query_id_android = queries_android.QueryId.astype(str).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2a2395",
   "metadata": {},
   "source": [
    "#### Pre-calculate the length of each document and store it in json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fe33f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the documents and get their length\n",
    "# document_length_android = {}\n",
    "# for docid in document_id_android:\n",
    "#     doc_vector = index_reader_android.get_document_vector(docid)\n",
    "#     document_length_android[docid] = sum(doc_vector.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d3a054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the doc lengths of each doc in a json file\n",
    "filename= 'hw2/ranker/document_length_android'\n",
    "# with open(filename, 'w') as outfile:\n",
    "#     json.dump(document_length_android, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "23343c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the doc lenghts from file\n",
    "with open(filename) as infile:\n",
    "    document_length_android = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f91c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_length_android)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c90da6",
   "metadata": {},
   "source": [
    "### Pre- calculate the average frequency of nouns in the queries and the documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba5061",
   "metadata": {},
   "source": [
    "#### This section precalculates the length of the documents and stores it in a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7e00b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queries_android_sample_list = list(set(queries_android_sample.QueryId.tolist()))\n",
    "len(queries_android_sample_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a33316eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loop over the queries and get their words and their tf \n",
    "all_queries_android={}\n",
    "# for queryid in query_id_android:\n",
    "for queryid in queries_android_sample_list:\n",
    "    query_vector = query_index_reader_android.get_document_vector(str(queryid))\n",
    "    all_queries_android[str(queryid)] = query_vector\n",
    "    \n",
    "len(all_queries_android.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7ffc86",
   "metadata": {},
   "source": [
    "#### Calculate average noun frequency for each query and document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e5cc549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_avg_noun_freq(text_id, searcher):\n",
    "    '''Calculate average noun frequency'''\n",
    "    # Fetch raw text of a document given its docid\n",
    "    raw = searcher.doc(text_id).raw()\n",
    "    # Get actual content from raw\n",
    "    content = json.loads(raw)['contents']\n",
    "    doc = nlp(content)\n",
    "\n",
    "    text_noun_count =0\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            text_noun_count = text_noun_count+1\n",
    "\n",
    "    avg_noun_freq = text_noun_count/len(doc)\n",
    "    return avg_noun_freq                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dedca926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the average noun frequency for the queries\n",
    "query_searcher = SimpleSearcher('hw2/ranker/android_query')\n",
    "\n",
    "q_noun_freq ={}\n",
    "for queryid in queries_android_sample_list:\n",
    "    q_noun_freq[str(queryid)] = get_avg_noun_freq(str(queryid), query_searcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c186f0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22998"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document_id_android)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28f7f9",
   "metadata": {},
   "source": [
    "#### For the documents get the words and their tf for each query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "566b64c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the average noun frequency for the documents\n",
    "# doc_searcher = SimpleSearcher('hw2/ranker/android_documents')\n",
    "# d_noun_freq ={}\n",
    "# for docid in document_id_android:\n",
    "#     d_noun_freq[str(docid)] = get_avg_noun_freq(str(docid), doc_searcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d855237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write document noun frequence to file\n",
    "filename= 'hw2/ranker/doc_noun_freq'\n",
    "# with open(filename, 'w') as outfile:\n",
    "#     json.dump(d_noun_freq, outfile)\n",
    "\n",
    "# read the document noun frequence from file\n",
    "with open(filename) as infile:\n",
    "    d_noun_freq = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f7aaf8",
   "metadata": {},
   "source": [
    "## Implementing the ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f63c846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76.56148360727019, 22998)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get total no.of documents and average document length\n",
    "N_android = index_reader_android.stats().get('non_empty_documents')\n",
    "avg_dl_android = index_reader_android.stats().get('total_terms')/ N_android\n",
    "avg_dl_android,N_android"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9e5a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomRanker(query_id, words_dict, k1=1.2, b=0.75, k3=1.2, k4= 1):\n",
    "    ''' Implements the custom ranker formula to score relevance\n",
    "    Input: \n",
    "        words_dict: It is a dict containing analyzed terms of a query as keys \n",
    "                    and its term freq in the query as value\n",
    "        hyperparameters: default values given\n",
    "                k1 : 1.2\n",
    "                b  : 0.75\n",
    "                k3 : 1.2\n",
    "                k4 : 1 [should be <= 1]\n",
    "    Output: \n",
    "        commom_term_bm25: A nested dict with doc_id as key and common terms \n",
    "                        between query and repective score as another dict\n",
    "                        Sample: {'doc_id1':{'how': 2.3, 'gain': 3},'doc_id2':{'tell': 2.3, 'level': 3} }\n",
    "    '''\n",
    "    common_term_ranker = {}\n",
    "    q_noun_f = q_noun_freq[query_id]\n",
    "    for word,qtf in words_dict.items():\n",
    "\n",
    "        #get the documents which contain the query term\n",
    "        postings_list = index_reader_android.get_postings_list(word, analyzer=None)\n",
    "        \n",
    "        if postings_list is not None :\n",
    "            word_df = len(postings_list)\n",
    "\n",
    "            for posting in postings_list:\n",
    "                doc_tf= posting.tf # c(w,d)\n",
    "                docsid = posting.docid\n",
    "                doc_id = index_reader_android.convert_internal_docid_to_collection_docid(docsid)\n",
    "                doc_len_android= document_length_android.get(doc_id) #get doc length\n",
    "                \n",
    "                # calculte the average noun frequency for each doc\n",
    "                doc_searcher = SimpleSearcher('hw2/ranker/android_documents')\n",
    "                d_noun_f = d_noun_freq.get(doc_id)\n",
    "\n",
    "                if doc_len_android:\n",
    "                #calculate the CustomRanker score for one word in query\n",
    "                    normalized_qtf = ((k3+1)* qtf)/(k3 + qtf)\n",
    "                    normalized_tf = ((k1 +1)*doc_tf)/((k1*(1-b+ b*(doc_len_android/avg_dl_android)))+doc_tf)\n",
    "                    variant_idf = np.log((N_android - word_df+0.5)/(word_df+0.5))\n",
    "                    context = k4 *(q_noun_f+1)/(d_noun_f +1)\n",
    "                    score = variant_idf * normalized_tf *normalized_qtf *context\n",
    "\n",
    "                    #check if doc_id is already in the dict\n",
    "                    if doc_id in common_term_ranker.keys():\n",
    "                        # summation of each word in query for a doc_id\n",
    "                        common_term_ranker[doc_id] = common_term_ranker[doc_id] + score \n",
    "                    else:\n",
    "                        common_term_ranker[doc_id] = score\n",
    "    return common_term_ranker       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25243f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rank_android = []\n",
    "# loop through the query ids\n",
    "for qid,words in all_queries_android.items():\n",
    "    result = CustomRanker(str(qid),words)\n",
    "    #sort the result as per score and take top 5 documents\n",
    "    documents_android_rank = dict(sorted(result.items(), key=lambda item: item[1], reverse=True)[:5]).keys()\n",
    "\n",
    "    # pair the document id with the query id\n",
    "    for doc_id in documents_android_rank:\n",
    "        final_rank_android.append([qid, doc_id])\n",
    "#     print(qid, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "373617cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 255 entries, 0 to 254\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   QueryId     255 non-null    object\n",
      " 1   DocumentId  255 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 4.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df_ranker =pd.DataFrame(final_rank_android, columns= ['QueryId', 'DocumentId'])\n",
    "\n",
    "# df_ranker.groupby('QueryId').size()\n",
    "df_ranker.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17234189",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ranker.to_csv('hw2/ranker/seffendi_ranker_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18d691",
   "metadata": {},
   "source": [
    "I approached the calculation of tf-idf similar to BM25, that is, I reused that portion with k1=1.2, b=0.75 and k3=1.2, because I wanted to get to the baseline score. After that I noticed that for the common terms the context in which the query and documents contain the common words is different. Since, pyserini uses stemmer we lose the context in which the word is being used. A word could be a noun in a query and used as a verb in the doc which gets a high relevance score when it actually shouldn't. So, to match the context in the query and documents I calculated the average noun frequency in the query and the average noun frequency in the document. If a query and document have similar terms and then have similar average frequency of nouns then probably the context will be the same. If the average frequency of nouns is less when tf is high then, maybe the term is being used as a verb and in that case it is not very relevant to the query.\n",
    "\n",
    "To calculate the score I took the ratio of the average frequency of noun in query with the average frequency of noun in document:\n",
    "\n",
    "context = (k4 \\*(q_noun_f+1))/(d_noun_f +1)\n",
    "\n",
    "I have added 1 in the numerator and denominator to avoid division by 0 or overall 0 value for the context variable. The value of k4 by default is 1 but can be tuned to account for less importance to average frequency of noun in query, since queries are short and can have higher avg noun frequency.\n",
    "\n",
    "For each query, document pair,\n",
    "\n",
    "score = variant_idf * normalized_tf *normalized_qtf *context\n",
    "\n",
    "Improvements: I think I could add terms to penalize the length of doc, which can result in higher average frequency of nouns but I could not get to a formula which resulted in beating baseline.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1450d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
